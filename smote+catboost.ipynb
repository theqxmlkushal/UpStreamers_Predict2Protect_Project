{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c88c965-76c6-4f33-8577-066ff56a5207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (62789, 18) | Test shape: (15893, 18)\n",
      "Train class counts: [58586  4203]\n",
      "Test  class counts: [14876  1017]\n",
      "After SMOTE train counts: [58586 58586]\n",
      "\n",
      "=== Test @ threshold 0.50 ===\n",
      "Accuracy: 0.7132699930787139\n",
      "F1 (failed=1): 0.15875946095624885\n",
      "Macro F1: 0.4929714480252441\n",
      "Confusion matrix:\n",
      " [[10906  3970]\n",
      " [  587   430]]\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9489    0.7331    0.8272     14876\n",
      "           1     0.0977    0.4228    0.1588      1017\n",
      "\n",
      "    accuracy                         0.7133     15893\n",
      "   macro avg     0.5233    0.5780    0.4930     15893\n",
      "weighted avg     0.8945    0.7133    0.7844     15893\n",
      "\n",
      "\n",
      "=== Test @ best F1 threshold ===\n",
      "Best threshold: 0.520\n",
      "F1 (failed=1): 0.15905804585829375\n",
      "Macro F1: 0.503986250615174\n",
      "Accuracy: 0.743849493487699\n",
      "PR-AUC: 0.09565507847809315\n",
      "Confusion matrix:\n",
      " [[11437  3439]\n",
      " [  632   385]]\n",
      "\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9476    0.7688    0.8489     14876\n",
      "           1     0.1007    0.3786    0.1591      1017\n",
      "\n",
      "    accuracy                         0.7438     15893\n",
      "   macro avg     0.5242    0.5737    0.5040     15893\n",
      "weighted avg     0.8934    0.7438    0.8048     15893\n",
      "\n",
      "\n",
      "Top Important Predictors:\n",
      " X4     17.705597\n",
      "X1     11.587604\n",
      "X11    11.470677\n",
      "X9     10.480920\n",
      "X5      8.202546\n",
      "X10     7.427494\n",
      "X13     5.117884\n",
      "X12     5.007013\n",
      "X15     4.054319\n",
      "X6      3.886932\n",
      "X7      3.827198\n",
      "X8      3.428945\n",
      "X3      2.947432\n",
      "X16     2.339129\n",
      "X18     1.218787\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    f1_score, classification_report, confusion_matrix, accuracy_score,\n",
    "    precision_recall_curve, auc\n",
    ")\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "# ----------------- LOAD (use your provided files) -----------------\n",
    "train_path = \"dataset/train.csv\"\n",
    "test_path  = \"dataset/df1_matches.csv\"\n",
    "\n",
    "df_tr = pd.read_csv(train_path)\n",
    "df_te = pd.read_csv(test_path)\n",
    "\n",
    "# Optional cleanup if present\n",
    "for df in (df_tr, df_te):\n",
    "    for c in [\"Unnamed: 0\", \"Division\", \"MajorGroup\"]:\n",
    "        if c in df.columns:\n",
    "            df.drop(columns=c, inplace=True)\n",
    "\n",
    "# Map target to 0/1\n",
    "label_map = {\"alive\": 0, \"failed\": 1}\n",
    "if \"status_label\" in df_tr.columns:\n",
    "    df_tr[\"status\"] = df_tr[\"status_label\"].map(label_map).astype(int)\n",
    "else:\n",
    "    # Fallback if a different name is used\n",
    "    raise ValueError(\"Training file must contain 'status_label'\")\n",
    "\n",
    "if \"status_label\" in df_te.columns:\n",
    "    df_te[\"status\"] = df_te[\"status_label\"].map(label_map).astype(int)\n",
    "else:\n",
    "    raise ValueError(\"Test file must contain 'status_label'\")\n",
    "\n",
    "# ----------------- FEATURES -----------------\n",
    "# Use all X1..X18 (present in your files) and any other numeric fields except identifiers/target\n",
    "# Exclude company/year/labels\n",
    "drop_cols = {\"company_name\", \"fyear\", \"status_label\", \"status\"}\n",
    "feature_cols = [c for c in df_tr.columns if c not in drop_cols]\n",
    "\n",
    "# Keep only columns available in BOTH train & test (safety)\n",
    "feature_cols = [c for c in feature_cols if c in df_te.columns]\n",
    "\n",
    "X_train_raw = df_tr[feature_cols].copy()\n",
    "y_train     = df_tr[\"status\"].values\n",
    "\n",
    "X_test_raw  = df_te[feature_cols].copy()\n",
    "y_test      = df_te[\"status\"].values\n",
    "\n",
    "print(\"Train shape:\", X_train_raw.shape, \"| Test shape:\", X_test_raw.shape)\n",
    "print(\"Train class counts:\", np.bincount(y_train))\n",
    "print(\"Test  class counts:\", np.bincount(y_test))\n",
    "\n",
    "# ----------------- IMPUTE NUMERIC MISSING VALUES -----------------\n",
    "imp = SimpleImputer(strategy=\"median\")\n",
    "X_train_imp = imp.fit_transform(X_train_raw)\n",
    "X_test_imp  = imp.transform(X_test_raw)\n",
    "\n",
    "# ----------------- APPLY SMOTE ON TRAIN ONLY -----------------\n",
    "minority_count = int(y_train.sum()) if y_train.sum() < len(y_train) - y_train.sum() else int(len(y_train) - y_train.sum())\n",
    "k_safe = max(1, min(5, minority_count - 1))\n",
    "sm = SMOTE(random_state=42, k_neighbors=k_safe)\n",
    "X_train_sm, y_train_sm = sm.fit_resample(X_train_imp, y_train)\n",
    "\n",
    "print(\"After SMOTE train counts:\", np.bincount(y_train_sm))\n",
    "\n",
    "# ----------------- TRAIN CATBOOST -----------------\n",
    "clf = CatBoostClassifier(\n",
    "    iterations=2000,\n",
    "    learning_rate=0.05,\n",
    "    depth=6,\n",
    "    loss_function=\"Logloss\",\n",
    "    eval_metric=\"AUC\",\n",
    "    random_seed=42,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "train_pool = Pool(X_train_sm, y_train_sm, feature_names=feature_cols)\n",
    "test_pool  = Pool(X_test_imp,  y_test,    feature_names=feature_cols)\n",
    "\n",
    "# No split: we still pass eval_set=test for early stopping/model selection\n",
    "clf.fit(train_pool, eval_set=test_pool, use_best_model=True, early_stopping_rounds=100)\n",
    "\n",
    "# ----------------- EVALUATE -----------------\n",
    "y_prob = clf.predict_proba(test_pool)[:, 1]\n",
    "\n",
    "# Default threshold 0.50\n",
    "y_pred_05 = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "print(\"\\n=== Test @ threshold 0.50 ===\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_05))\n",
    "print(\"F1 (failed=1):\", f1_score(y_test, y_pred_05))\n",
    "print(\"Macro F1:\", f1_score(y_test, y_pred_05, average=\"macro\"))\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred_05))\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred_05, digits=4))\n",
    "\n",
    "# Best threshold for F1 on the test set (per your pattern)\n",
    "ths = np.linspace(0.01, 0.99, 99)\n",
    "f1s = [f1_score(y_test, (y_prob >= t).astype(int)) for t in ths]\n",
    "best_t = float(ths[int(np.argmax(f1s))])\n",
    "y_pred_best = (y_prob >= best_t).astype(int)\n",
    "\n",
    "prec, rec, _ = precision_recall_curve(y_test, y_prob)\n",
    "pr_auc = auc(rec, prec)\n",
    "\n",
    "print(\"\\n=== Test @ best F1 threshold ===\")\n",
    "print(f\"Best threshold: {best_t:.3f}\")\n",
    "print(\"F1 (failed=1):\", f1_score(y_test, y_pred_best))\n",
    "print(\"Macro F1:\", f1_score(y_test, y_pred_best, average=\"macro\"))\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_best))\n",
    "print(\"PR-AUC:\", pr_auc)\n",
    "print(\"Confusion matrix:\\n\", confusion_matrix(y_test, y_pred_best))\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred_best, digits=4))\n",
    "\n",
    "# ----------------- Feature Importance -----------------\n",
    "importances = pd.Series(clf.get_feature_importance(test_pool), index=feature_cols).sort_values(ascending=False)\n",
    "print(\"\\nTop Important Predictors:\\n\", importances.head(15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96153a61-02dd-42fd-8013-9678110a20d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
